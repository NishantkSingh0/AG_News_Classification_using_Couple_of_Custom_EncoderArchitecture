{"cells":[{"cell_type":"markdown","source":["# **Import Necessary Libraries**\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"AveGWp4NMVnn"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sA5zTz32E-9j","executionInfo":{"status":"ok","timestamp":1723631343569,"user_tz":-330,"elapsed":31208,"user":{"displayName":"Nishant singh","userId":"09603408461863911910"}},"outputId":"429e7389-6c51-40c8-d7e0-20c7b642e0d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","import spacy"]},{"cell_type":"markdown","source":["# **Import Data**\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"1-z8vfgDMsPl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JsplPhYtFRUK","collapsed":true},"outputs":[],"source":["Data=pd.read_csv(\"/content/drive/MyDrive/AI/Dataset/AG_News-Classification/train.csv\")\n","Label=['World','Sports','Business','Sci/Tech']\n","nlp=spacy.load('en_core_web_sm')"]},{"cell_type":"markdown","source":["# **Collect Tokens from Dataset**\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"WGc9iixMMxl-"}},{"cell_type":"code","source":["Title=[]\n","num_rows=15000\n","Description=[]\n","desLen=0\n","Titlelen=0\n","for i in range(num_rows):\n","  if i%100==0:\n","    print(i)\n","  data=nlp(Data['Title'][i])\n","  Titlelen+=len(data)\n","  for j in data:\n","    Title.append(j.text)\n","  data2=nlp(Data['Description'][i])\n","  desLen+=len(data2)\n","  for k in data2:\n","    Description.append(k.text)"],"metadata":{"id":"SdXp-5GrU8is"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Index Collected Tokens Uniqly**\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"NQfnJsA7M3B4"}},{"cell_type":"code","source":["uniqTitle=set(Title)\n","uniqDescription=set(Description)"],"metadata":{"id":"ZTqaHxdUWoXj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Search word index in vocab**\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"bECrcxW8M8ca"}},{"cell_type":"code","source":["def search(vocab,word):\n","  if word in vocab:\n","    return vocab.index(word)\n","  else:\n","    return 0"],"metadata":{"id":"Nd04hiyBbFPu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Customly pad and create numerical Dataset Suitable for training**\n","\n","---\n","\n","\n","\n","---\n","  **Like:-**   \n","         [16918, 15949,  8012,  6428, 12855,  5790,  8017]\n"],"metadata":{"id":"AQqAIBD3NFRy"}},{"cell_type":"code","source":["def pad(seqlen,data,vocab,num_rows,increment=0):\n","  paddedData=[]\n","  for i in range(num_rows):\n","    A=nlp(data[i+increment])\n","    temp=[search(vocab,j.text) for j in A]\n","    if len(temp)<seqlen:\n","      temp+=[0]*(seqlen-len(temp))\n","      paddedData.append(temp)\n","    else:\n","      paddedData.append(temp[:seqlen])\n","  return np.array(paddedData)"],"metadata":{"id":"rkg-LcU6XTiT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Apply padding to Dataset**\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"MGXpEmLXNsSn"}},{"cell_type":"code","source":["AvgTitle=Titlelen//num_rows  #  7\n","AvgDescription=desLen//num_rows+1  #  38"],"metadata":{"id":"HkbVS_XXQVyz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["paddedTitle=pad(AvgTitle,Data['Title'],list(uniqTitle),15000)\n","paddedDescription=pad(AvgDescription,Data['Description'],list(uniqDescription),15000)"],"metadata":{"id":"sriDwPq_d3cT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Build Custom Designed Attention Layer with mask**\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"GAhAXuXoOK3W"}},{"cell_type":"code","source":["class Attention(tf.keras.layers.Layer):\n","  def __init__(self,num_heads,d_model,rate=0.1):\n","    super(Attention,self).__init__()\n","    self.d_model=d_model\n","    self.num_heads=num_heads\n","    assert d_model%num_heads==0,\"d_model must be divisible by num_heads\"\n","    self.depth=d_model//self.num_heads\n","    self.K=tf.keras.layers.Dense(d_model)\n","    self.Q=tf.keras.layers.Dense(d_model)\n","    self.V=tf.keras.layers.Dense(d_model)\n","    self.norm=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.dropout=tf.keras.layers.Dropout(rate)\n","\n","  def SplitHeads(self,X,batch_size):\n","    X=tf.reshape(X,(batch_size,-1,self.num_heads,self.depth))\n","    return tf.transpose(X,[0,2,1,3])\n","\n","  def ComputeAttention(self,Q,K,V,mask):\n","    matmul=tf.matmul(Q,K,transpose_b=True)\n","    dk=tf.cast(self.d_model,tf.float32)\n","    if mask is not None:\n","      matmul+=mask*-1e9\n","    soft=tf.nn.softmax(matmul/tf.math.sqrt(dk),axis=-1)\n","    output=tf.matmul(soft,V)\n","    return output,soft\n","\n","  def call(self,X,mask,training=False,split=False):\n","    batch_size=tf.shape(X)[0]\n","    K=self.K(X)\n","    Q=self.Q(X)\n","    V=self.V(X)\n","    if split is not True:\n","      K=self.SplitHeads(K,batch_size)\n","      Q=self.SplitHeads(Q,batch_size)\n","      V=self.SplitHeads(V,batch_size)\n","    output,soft=self.ComputeAttention(Q,K,V,mask)\n","    if split is not True:\n","      output=tf.transpose(output,[0,2,1,3])\n","      output=tf.reshape(output,(batch_size,-1,self.d_model))\n","    output=self.dropout(output,training=training)\n","    output=self.norm(output+X)\n","    return output,soft"],"metadata":{"id":"edfXIzGRe1Rp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Build a Pointwise Feed Forward Layer**\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"9iYCoUFBOZBn"}},{"cell_type":"code","source":["class PointwiseFFN(tf.keras.layers.Layer):\n","  def __init__(self,dff,d_model,rate=0.1):\n","    super(PointwiseFFN,self).__init__()\n","    self.dense1=tf.keras.layers.Dense(dff,activation='relu')\n","    self.dense2=tf.keras.layers.Dense(d_model)\n","    self.dropout=tf.keras.layers.Dropout(rate)\n","    self.norm=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","  def call(self,X,training=False):\n","    output=self.dense2(self.dense1(X))\n","    output=self.dropout(output,training=training)\n","    output=self.norm(output+X)\n","    return output"],"metadata":{"id":"_gLJolVBkCfU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Build Encoder layer By Attention and PFNN layer**\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"kEq6RdOOOd-g"}},{"cell_type":"code","source":["class EncoderLayer1(tf.keras.layers.Layer):\n","  def __init__(self,dff,d_model,num_heads,rate):\n","    super(EncoderLayer1,self).__init__()\n","    self.attention=Attention(num_heads,d_model,rate)\n","    self.ffn=PointwiseFFN(dff,d_model,rate)\n","\n","  def call(self,X,mask,training=False):\n","    output,soft=self.attention(X,mask,training=training)\n","    output=self.ffn(output,training=training)\n","    return output,soft"],"metadata":{"id":"Wr333Cd4duGy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EncoderLayer2(tf.keras.layers.Layer):\n","  def __init__(self,dff2,d_model2,num_heads2,rate):\n","    super(EncoderLayer2,self).__init__()\n","    self.attention=Attention(num_heads2,d_model2,rate)\n","    self.ffn=PointwiseFFN(dff2,d_model2,rate)\n","\n","  def call(self,X,mask,training=False):\n","    output,soft=self.attention(X,mask,training=training,split=True)\n","    output=self.ffn(output,training=training)\n","    return output,soft"],"metadata":{"id":"x6_u4hOAfOzw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Create Encoder Architecture by leveraging Embedding and positional Encoding**\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"3T5A0SeNO0a4"}},{"cell_type":"code","source":["class Encoder1(tf.keras.layers.Layer):\n","  def __init__(self,num_layers,dff,d_model,seq_len,num_heads,vocab_size,rate):\n","    super(Encoder1,self).__init__()\n","    self.num_layers=num_layers\n","    self.d_model=d_model\n","    self.embedding=tf.keras.layers.Embedding(vocab_size,d_model)\n","    self.PosEncoding=self.PEncoding(d_model,seq_len)\n","    self.layers=[EncoderLayer1(dff,d_model,num_heads,rate) for _ in range(num_layers)]\n","    self.dense1=tf.keras.layers.Dense(d_model,activation='relu')\n","\n","  def PEncoding(self,d_model,num_seq):\n","    angles=self.GetAngle(np.arange(num_seq)[:,np.newaxis],np.arange(d_model)[np.newaxis,:],d_model)\n","    angles[:,0::2]=np.sin(angles[:,0::2])\n","    angles[:,1::2]=np.cos(angles[:,1::2])\n","    angles=angles[np.newaxis,...]\n","    return tf.cast(angles,tf.float32)\n","  def GetAngle(self,pos,i,d_model):\n","    A=1/np.power(10000,2*(i//2)/np.float32(d_model))\n","    return pos*A\n","\n","  def call(self,input,mask,training=False):\n","    seq_len=tf.shape(input)[1]\n","    Attention_weights={}\n","    output=self.embedding(input)\n","    output*=tf.math.sqrt(tf.cast(self.d_model,tf.float32))\n","    output+=self.PosEncoding[:,:seq_len,:]\n","    for i in range(self.num_layers):\n","      output,soft=self.layers[i](output,mask,training=training)\n","      Attention_weights['encoder1_layer{}'.format(i+1)]=soft\n","    output=self.dense1(output)\n","    return output,Attention_weights"],"metadata":{"id":"72QtXAQ1g_Bq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Encoder2(tf.keras.layers.Layer):\n","  def __init__(self,dff2,d_model,d_model2,seq_len2,num_heads2,vocab_size2,rate):\n","    super(Encoder2,self).__init__()\n","    self.d_model=d_model2\n","    self.embedding=tf.keras.layers.Embedding(vocab_size2,d_model2)\n","    self.PosEncoding=self.PEncoding(d_model2,seq_len2)\n","    self.layers=EncoderLayer2(dff2,d_model2,num_heads2,rate)\n","    self.dense1=tf.keras.layers.Dense(d_model,activation='relu')\n","\n","  def PEncoding(self,d_model2,seq_len2):\n","    angles=self.GetAngle(np.arange(seq_len2)[:,np.newaxis],np.arange(d_model2)[np.newaxis,:],d_model2)\n","    angles[:,0::2]=np.sin(angles[:,0::2])\n","    angles[:,1::2]=np.cos(angles[:,1::2])\n","    angles=angles[np.newaxis,...]\n","    return tf.cast(angles,tf.float32)\n","\n","  def GetAngle(self,pos,i,d_model2):\n","    A=1/np.power(10000,2*(i//2)/np.float32(d_model2))\n","    return pos*A\n","\n","  def call(self,input,mask,training=False):\n","    seq_len=tf.shape(input)[1]\n","    Attention_weights={}\n","    output=self.embedding(input)\n","    output*=tf.math.sqrt(tf.cast(self.d_model,tf.float32))\n","    output+=self.PosEncoding[:,:seq_len,:]\n","    output,soft=self.layers(output,mask,training=training)\n","    Attention_weights['encoder2_layer1']=soft\n","    output=self.dense1(output)\n","    return output,Attention_weights"],"metadata":{"id":"7_D-CIuvokmA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Structure Model By Using External GAP1D and Feed forward layer**\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"JtGCx8LbPEqJ"}},{"cell_type":"code","source":["class Model(tf.keras.Model):\n","    def __init__(self, dff, dff2, d_model, d_model2, seq_len, seq_len2, num_heads, num_heads2, vocab_size, vocab_size2, num_layers, rate):\n","        super(Model, self).__init__()\n","        self.encoder1 = Encoder1(num_layers, dff, d_model, seq_len, num_heads, vocab_size, rate)\n","        self.encoder2 = Encoder2(dff2, d_model, d_model2, seq_len2, num_heads2, vocab_size2, rate)\n","        self.pool1 = tf.keras.layers.GlobalAveragePooling1D()\n","        self.pool2 = tf.keras.layers.GlobalAveragePooling1D()\n","        self.concat = tf.keras.layers.Concatenate()\n","        self.dense1 = tf.keras.layers.Dense(d_model, activation='relu')\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","        self.dense = tf.keras.layers.Dense(4, activation='softmax')\n","\n","    def call(self, input1, input2, mask, mask2,training=False):\n","        output1, Attention_weights1 = self.encoder1(input1, mask, training=training)\n","        output2, Attention_weights2 = self.encoder2(input2, mask2, training=training)\n","        output1 = self.pool1(output1)\n","        output2 = self.pool2(output2)\n","        output = self.concat([output1, output2])\n","        output = self.dense1(output)\n","        output = self.dropout(output, training=training)\n","        output = self.dense(output)\n","        return output, Attention_weights1, Attention_weights2"],"metadata":{"id":"8ipkyjtjqKJu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Define Model**\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"78iGOT9aPQpz"}},{"cell_type":"code","source":["dff=64\n","dff2=16\n","d_model=48\n","d_model2=12\n","num_heads=4\n","num_heads2=1\n","vocab_size=len(uniqDescription)\n","vocab_size2=len(uniqTitle)\n","num_layers=3\n","rate=0.1\n","seq_len=38\n","seq_len2=7\n","\n","model=Model(dff, dff2, d_model, d_model2, seq_len, seq_len2, num_heads, num_heads2, vocab_size, vocab_size2, num_layers, rate)"],"metadata":{"id":"GGfEonEsscEB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Define Loss function, Optimizer and Accuracy**\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"WmdDUkTWPVuU"}},{"cell_type":"code","source":["loss_object=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')\n","optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n","Accuracy=tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')"],"metadata":{"id":"LP2d592s1TFw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Build Mask Function**\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"mEc1S2wrPhpj"}},{"cell_type":"code","source":["def mask(seq):\n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","    return seq[:, tf.newaxis, tf.newaxis, :]\n","\n","def mask2(seq):\n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","    return seq[:, tf.newaxis, :]"],"metadata":{"id":"Qi2gXCOe11FY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Preprocess Title, description and Features for training**\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"NXgbVWACPokP"}},{"cell_type":"code","source":["Des=tf.convert_to_tensor(paddedDescription)\n","Titl=tf.convert_to_tensor(paddedTitle)\n","feat=tf.convert_to_tensor(Data['Class Index'][:num_rows]-1)\n","BUFFER_SIZE=num_rows\n","BATCH_SIZE=12\n","Dataset=tf.data.Dataset.from_tensor_slices((feat,Des,Titl))\n","Dataset=Dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder=True)"],"metadata":{"id":"p3dELurM5Qxa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Define Custom Training Loop with GradientTape**\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"DUar1Fn-P0PR"}},{"cell_type":"code","source":["@tf.function\n","def train_step(fea,des,titl):\n","\n","    Mask1=tf.convert_to_tensor(mask(des))\n","    Mask2=tf.convert_to_tensor(mask2(titl))\n","    with tf.GradientTape() as tape:\n","        predictions,weight1,weight2 = model(des,titl,Mask1, Mask2, training=True)\n","        loss = loss_object(fea, predictions)\n","\n","    gradients = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","    Accuracy(fea, predictions)\n","\n","    return loss\n"],"metadata":{"id":"D8oOPnUn3rGX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Train Model**\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"b0JkG5PlP8AX"}},{"cell_type":"code","source":["EPOCHS = 4\n","\n","for epoch in range(EPOCHS):\n","    print(f'Starting epoch {epoch+1}/{EPOCHS}')\n","    total_loss = 0.0000\n","\n","    for (batch, (fea,des,titl)) in enumerate(Dataset):\n","        batch_loss = train_step(fea,des,titl)\n","        temp_loss=np.sum(batch_loss)/BATCH_SIZE\n","        total_loss += temp_loss\n","\n","        if batch % 400 == 0:\n","            print(f'Epoch {epoch+1} || Batch {batch} || Loss {float(temp_loss):.4f} ~')\n","\n","    print(f'Epoch {epoch+1} Loss {float(np.sum(total_loss) / num_rows):.4f} Accuracy {Accuracy.result():.4f}')\n","    Accuracy.reset_state()\n"],"metadata":{"id":"Y5fiEXAv4bYy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723640618340,"user_tz":-330,"elapsed":216617,"user":{"displayName":"Nishant singh","userId":"09603408461863911910"}},"outputId":"bd124216-01fb-4eba-ec29-5f0de4d6e5c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting epoch 1/4\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py:609: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 || Batch 0 || Loss 1.6723 ~\n","Epoch 1 || Batch 400 || Loss 0.1289 ~\n","Epoch 1 || Batch 800 || Loss 0.3035 ~\n","Epoch 1 || Batch 1200 || Loss 0.2458 ~\n","Epoch 1 Loss 0.0473 Accuracy 0.7791\n","Starting epoch 2/4\n","Epoch 2 || Batch 0 || Loss 0.1703 ~\n","Epoch 2 || Batch 400 || Loss 0.5128 ~\n","Epoch 2 || Batch 800 || Loss 0.0991 ~\n","Epoch 2 || Batch 1200 || Loss 0.0667 ~\n","Epoch 2 Loss 0.0145 Accuracy 0.9439\n","Starting epoch 3/4\n","Epoch 3 || Batch 0 || Loss 0.0122 ~\n","Epoch 3 || Batch 400 || Loss 0.0034 ~\n","Epoch 3 || Batch 800 || Loss 0.1532 ~\n","Epoch 3 || Batch 1200 || Loss 0.0021 ~\n","Epoch 3 Loss 0.0060 Accuracy 0.9775\n","Starting epoch 4/4\n","Epoch 4 || Batch 0 || Loss 0.0016 ~\n","Epoch 4 || Batch 400 || Loss 0.0476 ~\n","Epoch 4 || Batch 800 || Loss 0.0025 ~\n","Epoch 4 || Batch 1200 || Loss 0.0007 ~\n","Epoch 4 Loss 0.0035 Accuracy 0.9870\n"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN10ySFNWp0LegQE/c2Uvbx"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}